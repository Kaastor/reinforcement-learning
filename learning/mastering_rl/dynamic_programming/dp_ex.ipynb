{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Przykład - dostawa towarów do food-trucka\n",
    "\n",
    "Problem: ile każdego dnia właściciel food trucka powinien kupić kotletów do burgera, aby wypełnić zapotrzebowanie. Food truck działa jedynie od poniedziałku do piątku.\n",
    "- Każdego ranka właściciel podejmuje decyzję o kupnie $A=\\{0, 100, 200, 300, 400\\}$ burgerów. Koszt pojedynczego wynosi $c=4$.\n",
    "- Pojemność lodówki wynosi $C=400$ burgerów. Każdy burger niewykorzystany w piątek zostaje wyrzucony.\n",
    "- Każde zakupione burgery, które przekraczają aktualną pojemność lodówki, zostają wyrzucone.\n",
    "- Codzienne zapotrzebowanie na burgery jest zmienną losową $D$ o następującym rozkładzie:\n",
    "![](img/D_prob_mass_fn.png)\n",
    "- Zarobek netto per burger wynosi $b=7$\n",
    "- Liczba sprzedanych dziennie burgerów wynosi $min(zapotrzebowanie, dostępne-burgery)$\n",
    "\n",
    "Cel: maksymalizacja tygodniowych zarobków $(b-c)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definicja środowiska"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Środowisko rozszerza klasę Env z biblioteki OpenAI Gym\n",
    "class FoodTruck(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.v_demand = [100, 200, 300, 400]\n",
    "        self.p_demand = [0.3, 0.4, 0.2, 0.1]\n",
    "        self.capacity = self.v_demand[-1]\n",
    "        self.days = ['Mon', 'Tue', 'Wed',\n",
    "                     'Thu', 'Fri', \"Weekend\"]\n",
    "        self.unit_cost = 4\n",
    "        self.net_revenue = 7\n",
    "        self.action_space = [0, 100, 200, 300, 400]\n",
    "        # stan środowiska - (dzień tygodnia, zapas burgerów na początku dnia)\n",
    "        # stan środowiska = obserwacja agenta (środowisko w pełni obserwowalne)\n",
    "        self.state_space = [(\"Mon\", 0)] \\\n",
    "                           + [(d, i) for d in self.days[1:]\n",
    "                              for i in [0, 100, 200, 300]]\n",
    "\n",
    "    # metoda obliczająca następny stan środowiska oraz nagrodę\n",
    "    def get_next_state_reward(self, state, action, demand):\n",
    "        day, inventory = state\n",
    "        result = {}\n",
    "        result['next_day'] = self.days[self.days.index(day)\n",
    "                                       + 1]\n",
    "        result['starting_inventory'] = min(self.capacity, inventory + action)\n",
    "        result['cost'] = self.unit_cost * action\n",
    "        result['sales'] = min(result['starting_inventory'], demand)\n",
    "        result['revenue'] = self.net_revenue * result['sales']\n",
    "        result['next_inventory'] = result['starting_inventory'] - result['sales']\n",
    "        result['reward'] = result['revenue'] - result['cost']\n",
    "        return result\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        next_s_r_prob = {}\n",
    "        for ix, demand in enumerate(self.v_demand):\n",
    "            result = self.get_next_state_reward(state,\n",
    "                                                action,\n",
    "                                                demand)\n",
    "            next_s = (result['next_day'], result['next_inventory'])\n",
    "            reward = result['reward']\n",
    "            prob = self.p_demand[ix]\n",
    "            if (next_s, reward) not in next_s_r_prob:\n",
    "                next_s_r_prob[next_s, reward] = prob\n",
    "            else:\n",
    "                next_s_r_prob[next_s, reward] += prob\n",
    "        return next_s_r_prob\n",
    "\n",
    "    # metody potrzebne do symulacji środowiska\n",
    "    def reset(self):\n",
    "        self.day = \"Mon\"\n",
    "        self.inventory = 0\n",
    "        state = (self.day, self.inventory)\n",
    "        return state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        day, inventory = state\n",
    "        if day == \"Weekend\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def step(self, action):\n",
    "        demand = np.random.choice(self.v_demand, p=self.p_demand)\n",
    "        result = self.get_next_state_reward((self.day, self.inventory),\n",
    "                                            action,\n",
    "                                            demand)\n",
    "        self.day = result['next_day']\n",
    "        self.inventory = result['next_inventory']\n",
    "        state = (self.day, self.inventory)\n",
    "        reward = result['reward']\n",
    "        done = self.is_terminal(state)\n",
    "        info = {'demand': demand, 'sales': result['sales']}\n",
    "        return state, reward, done, info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ewaluacja strategii (policy evaluation)\n",
    "\n",
    "Rozpatrzmy prostą strategię: na początku dnia, właściciel kupuje tyle burgerów, aby zapas był równy 200 lub 300 burgerów (każdą z opcji wybiera z równym prawdopodobieństwem ($0.5$)).\n",
    "Na przykład, jeżeli na początku dnia w lodówce jest 100 burgerów, zakupi on 100 lub 200 sztuk.\n",
    "\n",
    "Dokonajmy ewaluacji tej strategii."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# metoda zwracająca strategię działania\n",
    "def base_policy(states):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        day, inventory = s\n",
    "        prob_a = {}\n",
    "        if inventory >= 300:\n",
    "            prob_a[0] = 1\n",
    "        else:\n",
    "            prob_a[200 - inventory] = 0.5\n",
    "            prob_a[300 - inventory] = 0.5\n",
    "        policy[s] = prob_a\n",
    "    return policy  # dict: stan -> {akcja: prawdopodobieństwo}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# metoda zwracająca zaktualizowaną wartość dla stanu s\n",
    "def expected_update(env, v, s, prob_a, gamma):\n",
    "    expected_value = 0\n",
    "    for a in prob_a:\n",
    "        prob_next_s_r = env.get_transition_prob(s, a)\n",
    "        for next_s, r in prob_next_s_r:\n",
    "            expected_value += prob_a[a] * prob_next_s_r[next_s, r] * (r + gamma * v[next_s])\n",
    "    return expected_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# implementacja algorytmu 'ewaluacji strategii'\n",
    "def policy_evaluation(env, policy, max_iter=100, v = None, eps=0.1, gamma=1):\n",
    "    if not v:\n",
    "        v = {s: 0 for s in env.state_space}\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for s in v:\n",
    "            if not env.is_terminal(s):\n",
    "                v_old = v[s]\n",
    "                prob_a = policy[s]\n",
    "                v[s] = expected_update(env, v, s, prob_a, gamma)\n",
    "                max_delta = max(max_delta, abs(v[s] - v_old))\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(\"Converged in\", k, \"iterations.\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(\"Terminating after\", k, \"iterations.\")\n",
    "            break\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n",
      "Expected weekly profit: 2515.0\n",
      "The state values: {('Mon', 0): 2515.0, ('Tue', 0): 1960.0, ('Tue', 100): 2360.0, ('Tue', 200): 2760.0, ('Tue', 300): 3205.0, ('Wed', 0): 1405.0, ('Wed', 100): 1805.0, ('Wed', 200): 2205.0, ('Wed', 300): 2650.0, ('Thu', 0): 850.0000000000001, ('Thu', 100): 1250.0, ('Thu', 200): 1650.0, ('Thu', 300): 2095.0, ('Fri', 0): 295.00000000000006, ('Fri', 100): 695.0000000000001, ('Fri', 200): 1095.0, ('Fri', 300): 1400.0, ('Weekend', 0): 0, ('Weekend', 100): 0, ('Weekend', 200): 0, ('Weekend', 300): 0}\n"
     ]
    }
   ],
   "source": [
    "foodtruck = FoodTruck()\n",
    "policy = base_policy(foodtruck.state_space)\n",
    "v = policy_evaluation(foodtruck, policy)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])\n",
    "print(\"The state values:\", v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sprawdźmy, czy symulacja środowiska dla powyższej strategii da nam podobną wartość nagrody."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def choose_action(state, policy):\n",
    "    prob_a = policy[state]\n",
    "    action = np.random.choice(a=list(prob_a.keys()), p=list(prob_a.values()))\n",
    "    return action\n",
    "\n",
    "def simulate_policy(policy, n_episodes):\n",
    "    np.random.seed(0)\n",
    "    foodtruck = FoodTruck()\n",
    "    rewards = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = foodtruck.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        while not done:\n",
    "            action = choose_action(state, policy)\n",
    "            state, reward, done, info = foodtruck.step(action)\n",
    "            ep_reward += reward\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"Expected weekly profit:\", np.mean(rewards))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected weekly profit: 2518.1\n"
     ]
    }
   ],
   "source": [
    "simulate_policy(policy, 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otrzymana wartość jest bliska wartości wyznaczonej analitycznie!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iteracja strategii (policy iteration)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def policy_improvement(env, v, s, actions, gamma):\n",
    "    prob_a = {}\n",
    "    if not env.is_terminal(s):\n",
    "        max_q = np.NINF\n",
    "        best_a = None\n",
    "        for a in actions:\n",
    "            q_sa = expected_update(env, v, s, {a: 1}, gamma) # Aktualizacja wartości\n",
    "            if q_sa >= max_q:\n",
    "                max_q = q_sa\n",
    "                best_a = a\n",
    "        prob_a[best_a] = 1\n",
    "    else:\n",
    "        max_q = 0\n",
    "    print(prob_a, max_q, \"\\n\")\n",
    "    return prob_a, max_q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def policy_iteration(env,  eps=0.1, gamma=1):\n",
    "    np.random.seed(1)\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "    policy = {s: {np.random.choice(actions): 1}\n",
    "              for s in states} # 1. Inicjalizacja strategii\n",
    "    v = {s: 0 for s in states}\n",
    "    while True:\n",
    "        v = policy_evaluation(env, policy, v=v,\n",
    "                              eps=eps, gamma=gamma) # 2. Ewaluacja strategii\n",
    "        old_policy = policy\n",
    "        policy = {}\n",
    "        for s in states:\n",
    "            # 3. Aktualizacja wartości strategii\n",
    "            policy[s], _ = policy_improvement(env, v, s,\n",
    "                                              actions, gamma)\n",
    "        if old_policy == policy:\n",
    "            break\n",
    "    print(\"Optimal policy found!\")\n",
    "    return policy, v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n",
      "{300: 1} 1205.844 \n",
      "\n",
      "{300: 1} 932.64 \n",
      "\n",
      "{200: 1} 1332.64 \n",
      "\n",
      "{100: 1} 1732.64 \n",
      "\n",
      "{0: 1} 2132.6400000000003 \n",
      "\n",
      "{300: 1} 820.8000000000001 \n",
      "\n",
      "{200: 1} 1220.8000000000002 \n",
      "\n",
      "{100: 1} 1620.8000000000002 \n",
      "\n",
      "{0: 1} 2020.8000000000002 \n",
      "\n",
      "{100: 1} 689.9999999999999 \n",
      "\n",
      "{0: 1} 1089.9999999999998 \n",
      "\n",
      "{0: 1} 1424.0000000000002 \n",
      "\n",
      "{0: 1} 1546.0 \n",
      "\n",
      "{200: 1} 390.00000000000006 \n",
      "\n",
      "{100: 1} 790.0000000000001 \n",
      "\n",
      "{0: 1} 1190.0 \n",
      "\n",
      "{0: 1} 1400.0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "Converged in 6 iterations.\n",
      "{400: 1} 2583.0 \n",
      "\n",
      "{400: 1} 1982.9999999999998 \n",
      "\n",
      "{300: 1} 2383.0 \n",
      "\n",
      "{200: 1} 2783.0 \n",
      "\n",
      "{100: 1} 3183.0 \n",
      "\n",
      "{400: 1} 1494.0 \n",
      "\n",
      "{300: 1} 1894.0 \n",
      "\n",
      "{200: 1} 2294.0 \n",
      "\n",
      "{100: 1} 2694.0 \n",
      "\n",
      "{300: 1} 990.0 \n",
      "\n",
      "{200: 1} 1390.0 \n",
      "\n",
      "{100: 1} 1790.0 \n",
      "\n",
      "{0: 1} 2190.0 \n",
      "\n",
      "{200: 1} 390.00000000000006 \n",
      "\n",
      "{100: 1} 790.0000000000001 \n",
      "\n",
      "{0: 1} 1190.0 \n",
      "\n",
      "{0: 1} 1400.0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "Converged in 5 iterations.\n",
      "{400: 1} 2880.0 \n",
      "\n",
      "{400: 1} 2250.0 \n",
      "\n",
      "{300: 1} 2650.0 \n",
      "\n",
      "{200: 1} 3050.0 \n",
      "\n",
      "{100: 1} 3450.0 \n",
      "\n",
      "{400: 1} 1620.0 \n",
      "\n",
      "{300: 1} 2020.0 \n",
      "\n",
      "{200: 1} 2420.0 \n",
      "\n",
      "{100: 1} 2820.0 \n",
      "\n",
      "{300: 1} 990.0 \n",
      "\n",
      "{200: 1} 1390.0 \n",
      "\n",
      "{100: 1} 1790.0 \n",
      "\n",
      "{0: 1} 2190.0 \n",
      "\n",
      "{200: 1} 390.00000000000006 \n",
      "\n",
      "{100: 1} 790.0000000000001 \n",
      "\n",
      "{0: 1} 1190.0 \n",
      "\n",
      "{0: 1} 1400.0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "{} 0 \n",
      "\n",
      "Optimal policy found!\n",
      "Expected weekly profit: 2880.0\n",
      "{('Mon', 0): {400: 1}, ('Tue', 0): {400: 1}, ('Tue', 100): {300: 1}, ('Tue', 200): {200: 1}, ('Tue', 300): {100: 1}, ('Wed', 0): {400: 1}, ('Wed', 100): {300: 1}, ('Wed', 200): {200: 1}, ('Wed', 300): {100: 1}, ('Thu', 0): {300: 1}, ('Thu', 100): {200: 1}, ('Thu', 200): {100: 1}, ('Thu', 300): {0: 1}, ('Fri', 0): {200: 1}, ('Fri', 100): {100: 1}, ('Fri', 200): {0: 1}, ('Fri', 300): {0: 1}, ('Weekend', 0): {}, ('Weekend', 100): {}, ('Weekend', 200): {}, ('Weekend', 300): {}}\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_iteration(foodtruck)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])\n",
    "print(policy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Value iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iter=100, eps=0.1, gamma=1):\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "    v = {s: 0 for s in states}\n",
    "    policy = {}\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for s in states:\n",
    "            old_v = v[s]\n",
    "            policy[s], v[s] = policy_improvement(env,\n",
    "                                                 v,\n",
    "                                                 s,\n",
    "                                                 actions,\n",
    "                                                 gamma)\n",
    "            max_delta = max(max_delta, abs(v[s] - old_v))\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(\"Converged in\", k, \"iterations.\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(\"Terminating after\", k, \"iterations.\")\n",
    "            break\n",
    "    return policy, v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n",
      "Expected weekly profit: 2880.0\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration(foodtruck)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie\n",
    "\n",
    "1. Dokończ implementację metody *iteracja strategii (policy iteration)*.\n",
    "2. Opisz, na czym polega metoda *iteracja wartości (value iteration)*. Czym różni się ona od metody iteracja strategii?\n",
    "3. Podaj dwa powody, dla których zastosowanie programowania dynamicznego w praktyce jest trudne lub nawet niemożliwe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}