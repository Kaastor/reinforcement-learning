{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Programowanie dynamiczne\n",
    "\n",
    "- Programowanie dynamiczne zakłada pełną wiedzę o MDP (wewnętrznych procesach środowiska)\n",
    "- Daje optymalne rozwiązanie MDP\n",
    "- PD jest wykorzystane do *planowania* w MDP\n",
    "\n",
    "Dwa podstawowe problemy, które możemy rozwiązać przy pomocy planowania:\n",
    "- Prediction problem (policy evaluation)\n",
    "    - Input: MDP + policy\n",
    "    - Output: value function v\n",
    "- Control problem\n",
    "    - Input: MDP\n",
    "    - Output: optymalne value function oraz optymalne policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ewaluacja strategii (policy evaluation)\n",
    "\n",
    "Mamy dany MDP, strategię i zadajemy pytanie: jak dobra jest ta strategia?\n",
    "Aby wyznaczyć funkcję wartości V, użyjemy równania Bellman'a:\n",
    "\n",
    "| ![](img/bellman_expectation_v.png) |\n",
    "|:-:|\n",
    "| *Sutton, Barto, \"Reinforcement Learning an Introduction\", 2nd Edition, p. 74* |\n",
    "\n",
    "Algorytm:\n",
    "- Dla każdej k+1 iteracji,\n",
    "- Dla wszystkich stanów $s∈S$,\n",
    "- Zaktualizuj $v_{k+1}(s)$ z wartości $v_{k}(s')$, gdzie $s'$ jest następnikiem stanu $s$.\n",
    "\n",
    "| ![](img/it_policy_eval.png) |\n",
    "|:-:|\n",
    "| *Sutton, Barto, \"Reinforcement Learning an Introduction\", 2nd Edition, p. 75* |\n",
    "\n",
    "Wniosek: możemy użyć dowolnej wartości funkcji, aby znaleźć lepszą funkcję wartości jedynie dzięki lookup'om jeden krok w przód!\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iteracja strategii (policy iteration)\n",
    "\n",
    "Jak znaleźć najlepszą strategię? W dwóch krokach:\n",
    "\n",
    "1. Dokonaj ewaluacji strategii początkowej, aby wyznaczyć wartości funkcji,\n",
    "2. Ulepsz strategię poprzez wybór w każdym stanie akcji, która doprowadzi do stanu o największej wartości funkcji: $π'=greedy(v_{π})$\n",
    "3. Wróć do punktu 1., tym razem dla nowej strategii $π'$\n",
    "\n",
    "| ![](img/v_pi_relationship.png) |\n",
    "|:-:|\n",
    "| *Proces ten **zawsze** zbiega się do optymalnej strategii $π*$.* |\n",
    "\n",
    "| ![](img/policy_iteration.png) |\n",
    "|:-:|\n",
    "| *Sutton, Barto, \"Reinforcement Learning an Introduction\", 2nd Edition, p. 80* |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Value iteration\n",
    "\n",
    "The policy evaluation step of policy iteration can be truncated in several ways\n",
    "without losing the convergence guarantees of policy iteration.\n",
    "This algorithm is called value iteration. It can be written as a particularly simple update\n",
    "operation that combines the policy improvement and truncated policy evaluation steps:\n",
    "![](img/value_iteration.png)\n",
    "\n",
    "Notice the difference between a policy evaluation update and a value iteration\n",
    "update. The former selects the actions from a given policy, hence the\n",
    "$\\[ \\sum_{a}^{} π(a|s)\\]$ term in front of the expected update. The latter, on the other hand, does not follow a policy but actively searches for the best actions through the\n",
    "$\\[ max \\sum_{a}^{} ...\\]$ operator.\n",
    "\n",
    "Note that value iteration is obtained simply by turning the Bellman optimality equation into an update rule.\n",
    "\n",
    "Finally, let us consider how value iteration terminates. Like policy evaluation, value\n",
    "iteration formally requires an infinite number of iterations to converge exactly to $v_{*}$. In\n",
    "practice, we stop once the value function changes by only a small amount in a sweep.\n",
    "\n",
    "![](img/value_iteration_alg.png)\n",
    "\n",
    "Value iteration effectively combines, in each of its sweeps, **one sweep of policy evaluation and one sweep of policy improvement**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Materiały\n",
    "\n",
    "- Sutton, Barto, \"Reinforcement Learning an Introduction\", 2nd Edition, Chapter 4.\n",
    "- [RL Course by David Silver - Lecture 3: Planning by Dynamic Programming](https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=4)\n",
    "- Mastering Reinforcement Learning with Python, p. 132-151"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}