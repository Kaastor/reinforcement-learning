{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### The need for a complete model of the environment\n",
    "\n",
    "In the methods we have used so far, we have relied on the transition probabilities of the\n",
    "environment in our policy evaluation, policy iteration, and value iteration algorithms to\n",
    "obtain optimal policies. This is a luxury that we usually don't have in practice. It is either\n",
    "these probabilities are very difficult to calculate for each possible transition (which is often\n",
    "impossible to even enumerate), or we simply don't know them. You know what is much\n",
    "easier to obtain? A sample trajectory of transitions, either from the environment itself\n",
    "or from its simulation. In fact, simulation is a particularly important component in RL,\n",
    "as we will discuss separately towards the end of this chapter.\n",
    "Then the question becomes how we use sample trajectories to learn near-optimal policies.\n",
    "Well, this is exactly what we'll cover next in the rest of this chapter with Monte Carlo and\n",
    "TD methods. The concepts you will learn are at the center of many of the advanced RL\n",
    "algorithms.\n",
    "\n",
    "## Monte Carlo - model-free method\n",
    "\n",
    "We can estimate the state values and action values in an MDP from random samples.\n",
    "Monte Carlo (MC) estimation is a general concept that refers to making estimations through repeated random sampling. In the context of RL, it refers to\n",
    "**a collection of methods that estimates state values and action values using sample trajectories of complete episodes**.\n",
    "Using random samples is essential because often environment dynamics:\n",
    "- Is too complex to deal with\n",
    "- It is not known in the first place\n",
    "\n",
    "Summing up:\n",
    "- MC methods learn directly from episodes of experience\n",
    "- MC is *model-free*: no knowledge of MDP transitions/rewards\n",
    "- MC learns from *complete episodes*\n",
    "- MC uses the simplest idea: value = mean return\n",
    "- Caveat: can only apply MC to episodic MDPs (all episodes must terminate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monte Carlo prediction\n",
    "\n",
    "We need to be able to evaluate a given policy to be able to improve it. MC prediction suggests simply observing (many) sample trajectories, sequences of state-action-reward tuples, starting in $S$, to estimate expectation $v_\\pi(s) = E_{\\pi}[G_{t}|S_{t}=s]$.\n",
    "MV policy evaluation uses *empirical mean* return instead of *expected* return."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}