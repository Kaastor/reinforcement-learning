{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Dostępność modelu środowiska\n",
    "\n",
    "Metody oparte na **value iteration** czy **policy iteration** opierają się na wykorzystywaniu prawdopodobieństw przejść środowiska, aby wyznaczyć optymalne strategie.\n",
    "W praktyce bardzo rzadko mamy wiedzę o wewnętrznej dynamice środowiska lub jest ona bardzo trudna do wyznaczenia.\n",
    "\n",
    "Dużo prostszym zadaniem jest próbkowanie epizodów ze środowiska oraz użycie ich do wyznaczenia strategii bliskich optymalnym. Rodziną metod, które opierają się na tej idei, są metody Monte Carlo.\n",
    "\n",
    "\n",
    "## Monte Carlo - model-free method\n",
    "\n",
    "We can estimate the state values and action values in an MDP from random samples.\n",
    "Monte Carlo (MC) estimation is a general concept that refers to making estimations through repeated random sampling. In the context of RL, it refers to\n",
    "**a collection of methods that estimates state values and action values using sample trajectories of complete episodes**.\n",
    "Using random samples is essential because often environment dynamics:\n",
    "- Is too complex to deal with\n",
    "- It is not known in the first place\n",
    "\n",
    "Summing up:\n",
    "- MC methods learn directly from episodes of experience\n",
    "- MC is *model-free*: no knowledge of MDP transitions/rewards\n",
    "- MC learns from *complete episodes*\n",
    "- MC uses the simplest idea: value = mean return\n",
    "- Caveat: can only apply MC to episodic MDPs (all episodes must terminate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monte Carlo prediction\n",
    "\n",
    "We need to be able to evaluate a given policy to be able to improve it. MC prediction suggests simply observing (many) sample trajectories, sequences of state-action-reward tuples, starting in $S$, to estimate expectation $v_\\pi(s) = E_{\\pi}[G_{t}|S_{t}=s]$.\n",
    "MV policy evaluation uses *empirical mean* return instead of *expected* return.\n",
    "\n",
    "\n",
    "\n",
    "#### Przykład\n",
    "\n",
    "Robot umieszczony w środowisku typu *Grid World* otrzymuje nagrodę +1 za każdy ruch. Epizod kończy się, gdy robot wpadnie na ścianę. Strategia działania jest deterministyczna, lecz po wybraniu akcji, agent ma 70% szansy na poruszenie się w wybranym kierunku oraz po 10% szansy na każdy z pozostałych kierunków (dynamika środowiska).\n",
    "\n",
    "Rysunek poniżej przedstawia deterministyczną strategię $\\pi$ oraz dwie przykładowe trajektorie $\\tau$ przejścia agenta przez środowisko (start w (1,2), epizod kończy się po wpadnięciu na ścianę)\n",
    "\n",
    "![](img/example_robot.png)\n",
    "\n",
    "Aby wyznaczyć wartość oczekiwaną wartości $V_{\\pi}$ potrzebujemy (zgodnie z [definicją](https://pl.wikipedia.org/wiki/Warto%C5%9B%C4%87_oczekiwana)): sumy nagród z epizodu (*return*, $G_{\\tau}$) oraz prawdopodobieństwa wystąpienia danego epizodu ($p_{\\pi}(\\tau)$)\n",
    "\n",
    "#### $\\tau_{1}$\n",
    "\n",
    "- Kolejne stany: $S_{0}=(1,2), S_{1}=(1,1), S_{2}=(0,1), S_{3}=zderzenie$\n",
    "- Kolejne akcje: $A_{0}=dół, A_{1}=góra, A_{2}=prawo$\n",
    "- Kolejne nagrody: $R_{1}=1, R_{2}=1, R_{3}=0$\n",
    "- $p_{\\pi}(\\tau_{1})=0.7*0.1*0.1=0.007$\n",
    "- $G_{\\tau_{1}}=1+\\gamma 1+\\gamma^{2}0$\n",
    "-\n",
    "#### $\\tau_{2}$\n",
    "\n",
    "- Kolejne stany: $S_{0}=(1,2), S_{1}=(2,2), S_{2}=(2,1), S_{3}=(3,1), S_{4}=(2,1), S_{5}=(2,0), S_{6}=zderzenie$\n",
    "- Kolejne akcje: $A_{0}=dół, A_{1}=góra, A_{2}=prawo$\n",
    "- Kolejne nagrody: $R_{1}=1, R_{2}=1, R_{3}=0$\n",
    "- $p_{\\pi}(\\tau_{2})=0.1*0.7*0.1*0.7*0.1*0.1=0.000049$\n",
    "- $G_{\\tau_{2}}=1+\\gamma1+\\gamma^{2}1+\\gamma^{3}1+\\gamma^{4}1+\\gamma^{5}0$\n",
    "\n",
    "\n",
    "Aby wyznaczyć wartość stanu $s$, musimy wyznaczyć $v_{\\pi}=E_{\\pi}[G_{t}|S_{t}=s]=E_{\\pi}[G_{\\tau}|S_{\\tau}=s]=\\sum_{i}p_{\\pi}(\\tau_{i})*G_{\\tau_{i}}$\n",
    "Do powyższej formuły aplikujemy wszystkie epizody, które **rozpoczęły się w stanie $s$**.\n",
    "\n",
    "Wyznaczenie jej jest bardzo trudne, a nawet niemożliwe z uwagi na potencjalnie nieskończoną liczbę możliwych epizodów. Dlatego zamiast wartości teoretycznej $v_{\\pi}(s)$ będziemy używać estymacji tej wartości np.:\n",
    "$\\hat{v}_{\\pi}(1,2)=\\dfrac{G_{\\tau_{1}}+G_{\\tau_{2}}}{2}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Środowisko Food truck\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Środowisko rozszerza klasę Env z biblioteki OpenAI Gym\n",
    "class FoodTruck(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.v_demand = [100, 200, 300, 400]\n",
    "        self.p_demand = [0.3, 0.4, 0.2, 0.1]\n",
    "        self.capacity = self.v_demand[-1]\n",
    "        self.days = ['Mon', 'Tue', 'Wed',\n",
    "                     'Thu', 'Fri', \"Weekend\"]\n",
    "        self.unit_cost = 4\n",
    "        self.net_revenue = 7\n",
    "        self.action_space = [0, 100, 200, 300, 400]\n",
    "        # stan środowiska - (dzień tygodnia, zapas burgerów na początku dnia)\n",
    "        # stan środowiska = obserwacja agenta (środowisko w pełni obserwowalne)\n",
    "        self.state_space = [(\"Mon\", 0)] \\\n",
    "                           + [(d, i) for d in self.days[1:]\n",
    "                              for i in [0, 100, 200, 300]]\n",
    "\n",
    "    # metoda obliczająca następny stan środowiska oraz nagrodę\n",
    "    def get_next_state_reward(self, state, action, demand):\n",
    "        day, inventory = state\n",
    "        result = {}\n",
    "        result['next_day'] = self.days[self.days.index(day)\n",
    "                                       + 1]\n",
    "        result['starting_inventory'] = min(self.capacity, inventory + action)\n",
    "        result['cost'] = self.unit_cost * action\n",
    "        result['sales'] = min(result['starting_inventory'], demand)\n",
    "        result['revenue'] = self.net_revenue * result['sales']\n",
    "        result['next_inventory'] = result['starting_inventory'] - result['sales']\n",
    "        result['reward'] = result['revenue'] - result['cost']\n",
    "        return result\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        next_s_r_prob = {}\n",
    "        for ix, demand in enumerate(self.v_demand):\n",
    "            result = self.get_next_state_reward(state,\n",
    "                                                action,\n",
    "                                                demand)\n",
    "            next_s = (result['next_day'], result['next_inventory'])\n",
    "            reward = result['reward']\n",
    "            prob = self.p_demand[ix]\n",
    "            if (next_s, reward) not in next_s_r_prob:\n",
    "                next_s_r_prob[next_s, reward] = prob\n",
    "            else:\n",
    "                next_s_r_prob[next_s, reward] += prob\n",
    "        return next_s_r_prob\n",
    "\n",
    "    # metody potrzebne do symulacji środowiska\n",
    "    def reset(self):\n",
    "        self.day = \"Mon\"\n",
    "        self.inventory = 0\n",
    "        state = (self.day, self.inventory)\n",
    "        return state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        day, inventory = state\n",
    "        if day == \"Weekend\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def step(self, action):\n",
    "        demand = np.random.choice(self.v_demand, p=self.p_demand)\n",
    "        result = self.get_next_state_reward((self.day, self.inventory),\n",
    "                                            action,\n",
    "                                            demand)\n",
    "        self.day = result['next_day']\n",
    "        self.inventory = result['next_inventory']\n",
    "        state = (self.day, self.inventory)\n",
    "        reward = result['reward']\n",
    "        done = self.is_terminal(state)\n",
    "        info = {'demand': demand, 'sales': result['sales']}\n",
    "\n",
    "        return state, reward, done, info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Wybranie akcji dla danej strategii\n",
    "def choose_action(state, policy):\n",
    "    prob_a = policy[state]\n",
    "    action = np.random.choice(a=list(prob_a.keys()), p=list(prob_a.values()))\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# metoda zwracająca przykładową strategię działania\n",
    "def base_policy(states):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        day, inventory = s\n",
    "        prob_a = {}\n",
    "        if inventory >= 300:\n",
    "            prob_a[0] = 1\n",
    "        else:\n",
    "            prob_a[200 - inventory] = 0.5\n",
    "            prob_a[300 - inventory] = 0.5\n",
    "        policy[s] = prob_a\n",
    "    return policy  # dict: stan -> {akcja: prawdopodobieństwo}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### \"First-visit\" vs \"every-visit\" Monte Carlo\n",
    "\n",
    "Załóżmy, że chcemy estymować wartość stanu $v_{\\pi}(1,2)$. Na rysunku poniżej widzimy trzy trajektorie: $\\tau_{1}, \\tau_{2}, \\tau_{3}, \\tau_{4}$. Żaden z nich nie rozpoczyna się w punkcie $(1,2)$, ale mimo to możemy użyć trajektorii $\\tau_{1}, \\tau_{2}, \\tau_{3}$ do estymacji.\n",
    "Zauważmy, że $\\tau_{3}$ dwukrotnie przechodzi przez punkt $(1,2)$. Czy możemy użyć sumy nagród od pierwszej wizyty (first-visit) w tym punkcie, czy od każdej z wizyt (every-visit)?\n",
    "\n",
    "![](img/first_vs_every.png)\n",
    "\n",
    "Oba podejścia są poprawne. \"Every-visit\" jest prostszy w użyciu przez aproksymatory funkcji (jak sieci neuronowe)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def first_visit_return(returns, trajectory, gamma):\n",
    "    \"\"\"\n",
    "\n",
    "    :param returns: dict, klucze są stanami a wartością jest lista sum nagród uzyskanych z różnych trajektorii\n",
    "    :param trajectory: lista krotek: (stan, akcja, nagroda)\n",
    "    :param gamma: współczynnik dyskontowania\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    T = len(trajectory) - 1\n",
    "    for t, sar in enumerate(reversed(trajectory)):\n",
    "        s, a, r = sar\n",
    "        G = r + gamma * G\n",
    "        first_visit = True\n",
    "        for j in range(T - t):\n",
    "            if s == trajectory[j][0]:\n",
    "                first_visit = False\n",
    "        if first_visit:\n",
    "            if s in returns:\n",
    "                returns[s].append(G) # dodaje sumę nagród z trajektorii do listy\n",
    "            else:\n",
    "                returns[s] = [G]\n",
    "    return returns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def get_trajectory(env, policy):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    sar = [state]\n",
    "    while not done:\n",
    "        action = choose_action(state, policy)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        sar.append(action)\n",
    "        sar.append(reward)\n",
    "        trajectory.append(sar)\n",
    "        sar = [state]\n",
    "    return trajectory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def first_visit_mc(env, policy, gamma, n_trajectories):\n",
    "    np.random.seed(0)\n",
    "    returns = {}\n",
    "    v = {}\n",
    "    for i in range(n_trajectories):\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        returns = first_visit_return(returns,\n",
    "                                     trajectory,\n",
    "                                     gamma)\n",
    "    for s in env.state_space:\n",
    "        if s in returns:\n",
    "            v[s] = np.round(np.mean(returns[s]), 1)\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "foodtruck = FoodTruck()\n",
    "policy = base_policy(foodtruck.state_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "{('Mon', 0): 2515.9,\n ('Tue', 0): 1959.1,\n ('Tue', 100): 2362.2,\n ('Tue', 200): 2765.2,\n ('Wed', 0): 1411.3,\n ('Wed', 100): 1804.2,\n ('Wed', 200): 2198.9,\n ('Thu', 0): 852.9,\n ('Thu', 100): 1265.4,\n ('Thu', 200): 1644.4,\n ('Fri', 0): 301.1,\n ('Fri', 100): 696.5,\n ('Fri', 200): 1097.2}"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_est = first_visit_mc(foodtruck, policy, 1, 10000)\n",
    "v_est"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Widzimy, że estymowane wartości są bliskie wartościom rzeczywistym:\n",
    "\n",
    "![](img/v_pi_true_values.png)\n",
    "\n",
    "**Problem**, który się pojawia, to brak wyznaczonych wartości stanów dla stanów, gdzie agent na początku dnia wypełnia zasoby o 300 jednostek. Dzieje się tak, dlatego, że użyta strategia nie pozwala na odwiedzenie tych stanów!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monte Carlo control\n",
    "\n",
    "Problem sterowania MC polega na znalezieniu optymalnej strategii wykorzystując doświadczenie agenta (trajektorie). Aby mieć pewność, że każdy ze stanów zostanie odwiedzony, należy zaimplementować strategię **eksploracji**. W poniższym przykładzie wykorzystamy metodę $\\epsilon-greedy$.\n",
    "\n",
    "##### $\\epsilon-greedy$\n",
    "\n",
    "Metoda ta wybiera losową akcję z prawdopodobieństwem $\\epsilon$ oraz akcję maksymalizującą funkcję akcja-wartość z prawdopodobieństwem $1-\\epsilon$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Off-policy method\n",
    "Strategia wykorzystująca eksplorację nazywana jest **strategią zachowania (behavior policy), $b$**, natomiast strategia wykorzystująca jedynie najlepsze akcje (maksymalizacja funkcji akcja-wartość) nazywana jest **strategią docelową (target policy), $\\pi$**.\n",
    "Powyższa metoda nazywana jest **off-policy**, ponieważ korzysta z obu wspomnianych strategii. Pierwsza z nich używana jest do generowania danych (trajektorii), natomiast druga jest docelową, optymalną strategią, której szukamy.\n",
    "\n",
    "![](img/off_policy_mc_control.png)\n",
    "\n",
    "W metodach **on-policy** strategia zachowania jest używana do generacji danych oraz służy jako szukana strategia."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Przykład zwracanej wartości: {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01}\n",
    "# eps=0.05, najlepsza akcja=0\n",
    "def get_eps_greedy(actions, eps, a_best):\n",
    "    prob_a = {}\n",
    "    # Kod metody eps-greedy\n",
    "    return prob_a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import operator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_random_policy(states, actions):\n",
    "    policy = {}\n",
    "    n_a = len(actions)\n",
    "    for s in states:\n",
    "        policy[s] = {a: 1/n_a for a in actions}\n",
    "    return policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def off_policy_mc(env, n_iter, eps, gamma):\n",
    "    np.random.seed(0)\n",
    "    states =  env.state_space\n",
    "    actions = env.action_space\n",
    "    Q = {s: {a: 0 for a in actions} for s in states}\n",
    "    C = {s: {a: 0 for a in actions} for s in states}\n",
    "    target_policy = {}\n",
    "    behavior_policy = get_random_policy(states,\n",
    "                                        actions)\n",
    "    for i in range(n_iter):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "        trajectory = get_trajectory(env, behavior_policy)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for t, sar in enumerate(reversed(trajectory)):\n",
    "            s, a, r = sar\n",
    "            G = r + gamma * G\n",
    "            C[s][a] += W\n",
    "            Q[s][a] += (W/C[s][a]) * (G - Q[s][a])\n",
    "            a_best = max(Q[s].items(), key=operator.itemgetter(1))[0]\n",
    "            target_policy[s] = a_best\n",
    "            behavior_policy[s] = get_eps_greedy(actions, eps, a_best)\n",
    "            if a != target_policy[s]:\n",
    "                break\n",
    "            W = W / behavior_policy[s][a]\n",
    "    target_policy = {s: target_policy[s] for s in states\n",
    "                     if s in target_policy}\n",
    "    return target_policy, Q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10000\n",
      "Iteration: 20000\n",
      "Iteration: 30000\n",
      "Iteration: 40000\n",
      "Iteration: 50000\n",
      "Iteration: 60000\n",
      "Iteration: 70000\n",
      "Iteration: 80000\n",
      "Iteration: 90000\n",
      "Iteration: 100000\n",
      "Iteration: 110000\n",
      "Iteration: 120000\n",
      "Iteration: 130000\n",
      "Iteration: 140000\n",
      "Iteration: 150000\n",
      "Iteration: 160000\n",
      "Iteration: 170000\n",
      "Iteration: 180000\n",
      "Iteration: 190000\n",
      "Iteration: 200000\n",
      "Iteration: 210000\n",
      "Iteration: 220000\n",
      "Iteration: 230000\n",
      "Iteration: 240000\n",
      "Iteration: 250000\n",
      "Iteration: 260000\n",
      "Iteration: 270000\n",
      "Iteration: 280000\n",
      "Iteration: 290000\n"
     ]
    },
    {
     "data": {
      "text/plain": "{('Mon', 0): 400,\n ('Tue', 0): 400,\n ('Tue', 100): 300,\n ('Tue', 200): 200,\n ('Tue', 300): 100,\n ('Wed', 0): 400,\n ('Wed', 100): 300,\n ('Wed', 200): 200,\n ('Wed', 300): 100,\n ('Thu', 0): 300,\n ('Thu', 100): 200,\n ('Thu', 200): 100,\n ('Thu', 300): 0,\n ('Fri', 0): 200,\n ('Fri', 100): 100,\n ('Fri', 200): 0,\n ('Fri', 300): 0}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, Q = off_policy_mc(foodtruck, 300000, 0.05, 1)\n",
    "policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie\n",
    "\n",
    "1. Co należy zmienić w metodzie *first_visit_return*, aby zaimplementować metodę *every-visit*? Wynik $\\hat{v}$ powinien być identyczny z przypadkiem *first-visit*.\n",
    "2. Zaimplementuj metodę $\\epsilon-greedy$.\n",
    "3. Opisz metodę *importance sampling* wykorzystywaną w metodach Monte Carlo. Jaką rolę odgrywa ona w powyższej metodzie *off-policy*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Materiały\n",
    "\n",
    "- Sutton, Barto, \"Reinforcement Learning an Introduction\", 2nd Edition, Chapter 5.\n",
    "- [Monte Carlo And Off-Policy Methods](https://www.youtube.com/watch?v=bpUszPiWM7o)\n",
    "- Mastering Reinforcement Learning with Python, p. 152-163"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}