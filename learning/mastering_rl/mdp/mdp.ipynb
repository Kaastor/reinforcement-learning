{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## MDP - bringing the action\n",
    "\n",
    "An MDP is simply an MRP (Markov chain with rewards) with decisions affecting transition probabilities and potentially the rewards.\n",
    "An MDP is characterized by a〈S, A, P, R, y〉 tuple, where we have a finite set of actions, A, on top of the MRP.\n",
    "\n",
    "![](img/mdp.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our goal in an MDP is to find a policy that maximizes the expected cumulative reward.\n",
    "A policy simply tells which action(s) to take for a given state. In other words, it is\n",
    "a mapping from states to actions.\n",
    "![](img/policy_1.png)\n",
    "\n",
    "The policy of an agent potentially **affects** the **transition probabilities**, as well as the **rewards**, and it **fully defines the agent's behavior**. It is also stationary and does not change over time. Therefore, the dynamics of the MDP are defined by the following transition probabilities:\n",
    "![](img/policy_2.png)\n",
    "\n",
    "The policy also determines the transition probability matrix and the reward distribution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid world as an MDP\n",
    "\n",
    "Imagine that we can control the robot in our grid world, but only to some extent. In each step, we can take one of the following actions: up, down, left, and right. Then, the robot goes in the direction of the action with 70% chance and one of the other directions with\n",
    "10% chance each."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Goal: Again, our goal in RL is to figure out an optimal policy for the environment and the\n",
    "problem at hand that maximizes the expected discounted return."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### State-value function\n",
    "State-value function, is defined as the expected discounted return when starting in state S. Crucial point here: **the state-value function in an MDP is defined for a policy**. After all, **the transition probability matrix is determined by the policy**. Changing the policy is likely to lead to a different state-value function.\n",
    "![](img/sv_function.png)\n",
    "\n",
    "Bellman equation for v_π:\n",
    "![](img/sv_function_bellman.png)\n",
    "\n",
    "Bellman eq looks similar to MRP one, but is weighted by probability of taking the action as per the policy. (**dig deeper here why is that**)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Action-value function\n",
    "\n",
    "Assume that you have a policy, (not necessarily an optimal one). The policy already tells you which actions to take for each state with the associated probabilities, and you will follow that policy. However, for the current time step, you ask **\"what would be the expected cumulative return if I take action initially while in the current state, and follow thereafter for all states?\"**\n",
    "\n",
    "![](img/av_function.png)\n",
    "\n",
    "We can improve our policy by choosing the action that gives the highest action value for state S represented by argmax_a q_π(s,a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimal state-value and action-value functions\n",
    "\n",
    "An optimal policy is one that gives the optimal state-value function:\n",
    "![](img/optimal_sv_function.png)\n",
    "Optimal action-value function:\n",
    "![](img/optimal_av_function.png)\n",
    "\n",
    "The relationship between the optimal state-value and action-value functions is the following:\n",
    "![](img/sv_av_relationship.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bellman optimality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}