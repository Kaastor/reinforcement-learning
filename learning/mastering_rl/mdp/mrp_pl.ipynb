{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Łańcuch Markowa (Markov chain)\n",
    "\n",
    "Łańcuchy Markowa modelują procesy stochastyczne, które ewoluują w czasie zgodnie z wewnętrzną dynamiką.\n",
    "**Stan** jest to kompletny zbiór informacji, który opisuje sytuację, w jakiej znalazło się dane środowisko. Jeżeli *tranzycja* do kolejnego stanu, zależy jedynie od stanu aktualnego (nie rozpatrujemy pod uwagę tych stanów, w których środowisko już było wcześniej), mówimy, że proces posiada **własność Markowa**.\n",
    "![](img/markov_property.png)\n",
    "![](img/robot_markov_chain.png)\n",
    "Powyższy obrazek przedstawia zepsutego robota, który losowo porusza się po środowisku. Robot porusza się w zadanym kierunku zgodnie z prawdopodobieństwem określonym na rysunku. Będąc w stanie (1,2), nie ma znaczenia, jaką drogę przebył, aby się tam znaleźć. Przejście do kolejnego stanu zależy jedynie od aktualnego (własność Markowa).\n",
    "![](img/absorbing.png)\n",
    "Załóżmy, że jeżeli robot uderzy w ścianę (tj. poruszy się w jej kierunku) rozbija się i nie może się już poruszyć. Taki stan nazywamy *terminalnym*, który kończy *epizod*.\n",
    "\n",
    "## Proces nagród Markowa (Markov reward process)\n",
    "\n",
    "Każdy system posiada stany, w których lepiej byłoby się znajdować i takie, w których mniej. Łańcuch Markowa nie mówi nam nic na temat przewagi jednych stanów nad drugimi. Do tego potrzebujemy **nagród** przypisanych do poszczególnych stanów procesu.\n",
    "\n",
    "### Przykład\n",
    "\n",
    "Rozpatrzmy przykładowe środowisko typu *grid world* o wielkości 3x3 (stany indeksowane są następująco: (0,0):1, (0,1):2, ... , (2,2):9) ze stanem terminalnym, do którego agent przechodzi, gdy uderzy w ścianę."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "m = 3\n",
    "m2 = m ** 2  # liczba stanów\n",
    "q = np.zeros(m2)\n",
    "q[m2 // 2] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# funkcja tworząca macierz przejść z określonymi prawdopodobieństwami tranzycji pomiędzy stanami\n",
    "def get_P(m, p_up, p_down, p_left, p_right):\n",
    "    m2 = m ** 2\n",
    "    P = np.zeros((m2, m2))\n",
    "    ix_map = {i + 1: (i // m, i % m) for i in range(m2)}\n",
    "    for i in range(m2):\n",
    "        for j in range(m2):\n",
    "            r1, c1 = ix_map[i + 1]\n",
    "            r2, c2 = ix_map[j + 1]\n",
    "            rdiff = r1 - r2\n",
    "            cdiff = c1 - c2\n",
    "            if rdiff == 0:\n",
    "                if cdiff == 1:\n",
    "                    P[i, j] = p_left\n",
    "                elif cdiff == -1:\n",
    "                    P[i, j] = p_right\n",
    "                elif cdiff == 0:\n",
    "                    if r1 == 0:\n",
    "                        P[i, j] += p_down\n",
    "                    elif r1 == m - 1:\n",
    "                        P[i, j] += p_up\n",
    "                    if c1 == 0:\n",
    "                        P[i, j] += p_left\n",
    "                    elif c1 == m - 1:\n",
    "                        P[i, j] += p_right\n",
    "            elif rdiff == 1:\n",
    "                if cdiff == 0:\n",
    "                    P[i, j] = p_down\n",
    "            elif rdiff == -1:\n",
    "                if cdiff == 0:\n",
    "                    P[i, j] = p_up\n",
    "    return P\n",
    "\n",
    "# Poszerzenie macierzy P o stan terminalny 'crashed'\n",
    "P = np.zeros((m2 + 1, m2 + 1))\n",
    "P[:m2, :m2] = get_P(3, 0.2, 0.3, 0.25, 0.25)\n",
    "for i in range(m2):\n",
    "    P[i, m2] = P[i, i]\n",
    "    P[i, i] = 0\n",
    "P[m2, m2] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.  , 0.25, 0.  , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.55],\n       [0.25, 0.  , 0.25, 0.  , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.3 ],\n       [0.  , 0.25, 0.  , 0.  , 0.  , 0.2 , 0.  , 0.  , 0.  , 0.55],\n       [0.3 , 0.  , 0.  , 0.  , 0.25, 0.  , 0.2 , 0.  , 0.  , 0.25],\n       [0.  , 0.3 , 0.  , 0.25, 0.  , 0.25, 0.  , 0.2 , 0.  , 0.  ],\n       [0.  , 0.  , 0.3 , 0.  , 0.25, 0.  , 0.  , 0.  , 0.2 , 0.25],\n       [0.  , 0.  , 0.  , 0.3 , 0.  , 0.  , 0.  , 0.25, 0.  , 0.45],\n       [0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.25, 0.  , 0.25, 0.2 ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.25, 0.  , 0.45],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# końcowa macierz tranzycji, ze stanu 9 agent nie może już przejść do żadnego innego poza stanem 9.\n",
    "# Ze stanu pierwszego istnieje 0.55 szansy, że agent uderzy w ścianę, 0.25, że przejdzie w prawo (stan 2) oraz\n",
    "# 0.2 szansy że pójdzie do góry (stan 4)\n",
    "P"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agent otrzymuje nagrodę 1 za każdy ruch. Gdy wpadnie w ścianę, epizod się kończy, a agent otrzymuje nagrodę równą 0.\n",
    "Agent rozpoczyna epizod w stanie początkowym, a następne zdobyte przez agenta nagrody są zapisywane.\n",
    "Pozwoli to później na określenie średniej (oczekiwanej) wartości **sumy nagród** (*return*, suma nagród w danym epizodzie) dla każdego ze stanów środowiska.\n",
    "Taką wartość nazywamy **wartością stanu (state value)**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "n = 10 ** 5\n",
    "avg_rewards = np.zeros(m2)\n",
    "for s in range(9): # dla każdego stanu początkowego\n",
    "    for i in range(n): # symulacja n epizodów\n",
    "        crashed = False\n",
    "        s_next = s\n",
    "        episode_reward = 0\n",
    "        while not crashed: # symulacja przejść, dopóki agent nie wpadnie w ścianę\n",
    "            s_next = np.random.choice(m2 + 1, p=P[s_next, :])\n",
    "            if s_next < m2: # wewnątrz ścian\n",
    "                episode_reward += 1\n",
    "            else: # zderzenie ze ścianą\n",
    "                crashed = True\n",
    "        avg_rewards[s] += episode_reward\n",
    "avg_rewards /= n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.46, 2.13, 1.46, 2.45, 3.42, 2.43, 1.99, 2.82, 1.98])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(avg_rewards, 2) # średnie sumy nagród"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zadanie #1\n",
    "\n",
    "Który stan jest najlepszy, a który najgorszy i dlaczego? Z czego wynika przewaga jednego nad drugim?\n",
    "\n",
    "### Zadanie #2\n",
    "\n",
    "Wartość stanu definiowana jest jako oczekiwana zdyskontowana suma nagród, gdy zaczynamy ze stanu *s*:\n",
    "![](img/sv_mrp.png)\n",
    "Co oznacza **współczynnik dyskontowania γ**? Jak go możemy interpretować?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rekurencyjna relacja pomiędzy wartościami stanów\n",
    "\n",
    "Możemy wyznaczyć wartość stanu ze stanów sąsiadujących (takich, do których agent może przejść ze stanu rozpatrywanego) zgodnie z formułą rekurencyjną:\n",
    "![](img/recursive_relationship_formula.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "3.4205"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estymacja wartości (state value) czwartego stanu z wartości stanów sąsiadujących\n",
    "(1 + 2.45) * 0.25 + (1 + 2.44) * 0.25 + 0.2 * (1+2.81) + 0.3*(1+2.12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Powyższą formułę nazywamy **równaniem Bellmana dla MRP** i możemy ją zapisać następująco (uwzględniając już wsp. dyskontowania):\n",
    "![](img/bellman_eq_mrp.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zadanie #3\n",
    "\n",
    "Wyznacz używając reguły rekurencyjnej wartości stanów dla pozostałych stanów środowiska (v(0,0), v(1,0), ..., v(2,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iteracyjna estymacja wartości stanów\n",
    "\n",
    "Jedną z centralnych idei UzW jest iteracyjna możliwość estymacji wartości stanów.\n",
    "Użyjmy równania Bellmana jako reguły aktualizacji wartości.\n",
    "Estymację kończymy, gdy aktualizacja dla wartości stanu jest mniejsza niż zadana wartość progowa (threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def estimate_state_values(P, m2, threshold):\n",
    "    v = np.zeros(m2 + 1)\n",
    "    max_change = threshold\n",
    "    terminal_state = m2\n",
    "    while max_change >= threshold:\n",
    "        max_change = 0\n",
    "        for s in range(m2 + 1): # dla każdego stanu\n",
    "            v_new = 0\n",
    "            for s_next in range(m2 + 1): # aktualizacja wartości stanów\n",
    "                r = 1 * (s_next != terminal_state)\n",
    "                v_new += P[s, s_next] * (r + v[s_next])\n",
    "            max_change = max(max_change, np.abs(v[s] - v_new))\n",
    "            v[s] = v_new\n",
    "    return np.round(v, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.47, 2.12, 1.47, 2.44, 3.42, 2.44, 1.99, 2.82, 1.99, 0.  ])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_state_values(P, m2, 0.005)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zadanie #4\n",
    "Dlaczego to podejście jest niepraktyczne w praktyce? Podaj dwa powody.\n",
    "\n",
    "*Tip: ma to związek z przestrzenią stanów oraz macierzą P.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zadanie #5\n",
    "\n",
    "Dokonaj estymacji wartości stanu dla środowisk o rozmiarach:\n",
    "* 10x10\n",
    "* 50x50\n",
    "* 100x100\n",
    "* 500x500\n",
    "* 1000x1000\n",
    "\n",
    "Zmierz czas obliczeń i przedstaw wyniki w tabeli. Opisz obserwacje."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Materiały\n",
    "\n",
    "1. Mastering RL with Python, str 104-121."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO\n",
    "\n",
    "- Dodać barto i silvera,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}