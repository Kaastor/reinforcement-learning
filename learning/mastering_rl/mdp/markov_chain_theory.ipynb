{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "Markov Decision Process (MDP) is the framework we use to model these sequential decision-making problems. Building on that theory, Dynamic Programming (DP) is the field that proposes solution methods for MDPs. RL, in some sense, is a collection of approximate DP approaches that enable us to obtain good (but not necessarily optimal) solutions to very complex problems that are unfeasible to solve with exact DP methods.\n",
    "\n",
    "In an MDP, the actions an agent takes have long-term consequences, which is what differentiates it from the Multi-Armed Bandit (MAB) problems"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Markov Chains\n",
    "\n",
    "They only model a special type of stochastic processes that are governed by some internal transition dynamics.\n",
    "\n",
    "A Markov chain is usually depicted using a directed graph. A Markov chain diagram for the robot example in a 2x2 grid world:\n",
    "![](img/markov_chain.png)\n",
    "\n",
    "**Tip**: Many systems can be made Markovian by including historical information in the state. Consider a modified robot example where the robot is more likely to continue in the direction it moved in the previous time step. Although such a system seemingly does not satisfy the Markov property, we can simply redefine the state to include the visited cells over the last two time steps, such as = x_t = (s_t , s_t−1 ) = ((0,1), (0,0)) . The transition probabilities would be independent of the past states under this new state definition and the Markov property\n",
    "would be satisfied.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Classification of states in a Markov chain\n",
    "\n",
    "If the environment can transition from state *i* to state *j* after some number of steps with a positive probability, we say *j* is **reachable** from *i*. If *i* is also reachable from *j*, those states are said to **communicate**. If all the states in a Markov chain **communicate** with each other, we say that the Markov chain is **irreducible**, which is what we had in our robot example.\n",
    "\n",
    "A state *s* is an **absorbing** state if the only possible transition is to itself, which is *P(s_t+1=s | s_t=s )=1*. An absorbing state is equivalent to a **terminal** state that marks the end of an **episode**. In addition to terminal states, an episode can also terminate after a time limit *T*.\n",
    "![](img/absorbing.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transient and recurrent states\n",
    "\n",
    "A state is called a **transient** state, if there is another state ′ , that is reachable from ,\n",
    "but not vice versa. Provided enough time, an environment will eventually move away from\n",
    "transient states and never come back.\n",
    "![](img/transient.png)\n",
    "So, wherever the robot is on the light side, it will eventually transition into the dark side and won't be able to come back. All the states on the light side are transient.\n",
    "Finally, a state that is not transient is called a **recurrent** state. The states on the dark side are recurrent in this example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Periodic and aperiodic states\n",
    "\n",
    "We call a state, s, periodic if all of the paths leaving s come back after some multiple\n",
    "of k > 1 steps. Consider the example in Figure 4.5, where all the states have a period\n",
    "of k = 4 :\n",
    "\n",
    "![](img/periodic.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ergodicity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transitionary and steady state behavior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Semi-Markov processes and continuous-time Markov chains\n",
    "\n",
    "All of the examples and formulas we have provided so far are related to discrete-time\n",
    "Markov chains, which are environments where transitions occur at discrete time steps,\n",
    "such as every minute or every 10 seconds. But in many real-world scenarios, when the\n",
    "next transition will happen is also random, which makes them a semi-Markov process.\n",
    "In those cases, we are usually interested in predicting the state after amount of time\n",
    "(rather than after steps).\n",
    "\n",
    "One example of a scenario where a time component is important is queuing systems – for\n",
    "instance, the number of customers waiting in a customer service line. A customer could\n",
    "join the queue anytime and a representative could complete the service with a customer at\n",
    "any time – not just at discrete time steps. Another example is a work-in-process inventory\n",
    "waiting in front of an assembly station to be processed in a factory. In all these cases,\n",
    "analyzing the behavior of the system over time is very important to be able to improve the\n",
    "system and take action accordingly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}