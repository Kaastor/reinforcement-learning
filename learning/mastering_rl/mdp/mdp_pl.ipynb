{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## MDP - Proces Decyzyjny Markowa\n",
    "\n",
    "MDP jest procesem nagród Markowa (MRP) z dodanymi decyzjami, które mają wpływ na prawdopodobieństwa przejść. Charakteryzowany jest przez piątkę〈S, A, P, R, y〉, gdzie:\n",
    "- S - zbiór stanów\n",
    "- A - zbiór akcji\n",
    "- P - macierz przejść\n",
    "- R - funkcja nagrody\n",
    "- współczynnik dyskontowania y\n",
    "\n",
    "![](img/mdp.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Celem w MDP jest znalezienie strategii (policy), która maksymalizuje oczekiwaną sumę nagród.\n",
    "Strategia mówi agentowi, jaką akcję (lub akcje) podjąć w danym stanie.\n",
    "Bardziej formalnie, strategia jest rozkładem prawdopodobieństwa akcji dla danego stanu:\n",
    "![](img/policy_1.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funkcja wartości Stanu (State-value function)\n",
    "Funkcja ta jest zdefiniowana jako oczekiwana suma nagród, gdy rozpoczynamy w stanie S.\n",
    "Wartość funkcji stanu jest zdefiniowana **dla konkretnej strategii**. Zmiana strategii spowoduje zmianę wartości funkcji.\n",
    "![](img/sv_function.png)\n",
    "\n",
    "Równanie Bellman'a dla v_π:\n",
    "![](img/sv_function_bellman.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funkcja wartości Akcji (Action-value function)\n",
    "\n",
    "Załóżmy, że posiadamy strategię (nie koniecznie optymalną) oraz kierujemy się jej decyzjami. Mówi ona, jakie akcje podjąć w każdym ze stanów wraz z ich prawdopodobieństwami.\n",
    "Pytanie na jakie odpowiada funkcja wartości akcji to: **jaka będzie oczekiwana suma nagród jeżeli wybiorę akcję *a* w stanie *s* oraz będę dalej kierował się wybraną strategią?**\n",
    "\n",
    "![](img/av_function.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie\n",
    "Dla wybranych trzech stanów ze środowiska z poprzednich zajęć ułóż przykładową (probabilistyczną) strategię.\n",
    "Następnie wyznacz wartości funkcji v_π oraz q_π (na podstawie wcześniej wyznaczonej v_π) dla zdefiniowanej strategii.\n",
    "Załóż, że wartości początkowe v_π mają wartości jak na poprzednich zajęciach (rysunek poniżej).\n",
    "![](img/avg_returns.png)\n",
    "\n",
    "### TODO\n",
    "\n",
    "- Dodać barto i silvera, ogólnie materiały\n",
    "- Lepiej sformułować zadanie, rozrysować konkretny MDP i dla niego dać zadanie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}